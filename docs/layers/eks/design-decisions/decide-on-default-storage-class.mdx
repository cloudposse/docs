---
title: "Decide on Default Storage Class for EKS Clusters"
sidebar_label: "Default Storage Class"
description: Determine the default storage class for Kubernetes EKS clusters
---
import Intro from '@site/src/components/Intro';
import KeyPoints from '@site/src/components/KeyPoints';

<Intro>
When provisioning EKS (Kubernetes) clusters, selecting the appropriate default storage class is crucial for ensuring optimal performance, scalability, and cost-efficiency of persistent storage for workloads.
</Intro>

## Default Storage Class Options

We need to decide between **Amazon EFS (Elastic File System)** and **Amazon EBS (Elastic Block Store)** as the default storage class for our EKS clusters.

<KeyPoints>
- Availability Zone Lock-in: **EBS:** EBS volumes are locked to a single Availability Zone, which may affect high availability and disaster recovery strategies. This is the primary argument against EBS. However, sharing data across Pods is an uncommon use case.
- Performance: **EFS:** Can be mitigated by paying for additional bandwidth, but has routinely caused outages due to throttling even with low-performance applications. Additionally, poor lock performance makes EFS completely unsuitable for high-performance applications like RDBMS.
- Cost: **EFS:** Significantly more expensive than EBS, at least 3x the price per GB and potentially more depending on performance demands, although there may be some savings from not having to reserve size for future growth.
</KeyPoints>

## Amazon EFS

**Amazon EFS** provides a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.

### Pros:
- **Scalability:** Automatically scales storage capacity as needed without manual intervention.
- **Shared Access:** Allows multiple pods to access the same file system concurrently, facilitating shared storage scenarios.
- **Managed Service:** Fully managed by AWS, reducing operational overhead for maintenance and scaling.
- **NFS Protocol Support:** Compatible with applications requiring NFS (Network File System) access.

### Cons:
- **Performance:** Generally offers lower performance compared to EBS, with throughput as low as 100 MB/s, which may not meet the demands of even modestly demanding applications.
- **Cost:** Significantly more expensive than EBS, at least 3x the price per GB and potentially more depending on performance demands, although there may be some savings from not having to reserve size for future growth.
- **Latency:** Higher latency compared to EBS, which may impact performance-sensitive applications.

## Amazon EBS

**Amazon EBS** provides high-performance block storage volumes for use with Amazon EC2 instances, suitable for a wide range of workloads.

### Pros:
- **Performance:** Offers high IOPS and low latency, making it ideal for performance-critical applications.
- **Cost-Effective:** Potentially lower costs for specific storage types and usage scenarios.
- **Integration:** Well-integrated with Kubernetes through the EBS CSI (Container Storage Interface) driver, facilitating seamless provisioning and management.
- **Snapshot and Backup:** Supports snapshotting for data backup, recovery, and cloning.

### Cons:
- **Single-Attach Limitation:** EBS volumes may only be attached to a single node at a time, limiting shared access across multiple pods.
- **Data Sharing:** Data cannot be easily shared across multiple instances, requiring additional configurations or solutions for shared access.
- **Availability Zones:** EBS volumes are confined to a single Availability Zone, which may affect high availability and disaster recovery strategies. Kubernetes has implemented several mitigations for this limitation, and ECS likely has similar solutions.
- **Scalability:** Requires manual provisioning and management of storage capacity, which can introduce operational complexity.

## Recommendations

Use **Amazon EBS** unless there is a good reason not to.

For example, use Amazon EFS when:

- **Multiple Pods need read/write access to the same data at the same time.**
- **Persistent data needs to be available in multiple Availability Zones and the underlying application does not support replication.**
